[[{"l":"The Green Alliance"},{"l":"Our Mission","p":["Our mission is to provide beginner-friendly, easy-to-implement documentation and articles to help you implement hand written vision into your FIRST team's robot."]},{"i":"what-is-computer-vision","l":"What is Computer Vision?","p":["Computer vision is the technology that allows computers to analyze, interpret, and understand visuals like humans do. It can involve various tasks ranging from spotting a saturated object to determining where something is in its environment. Computers complete these tasks by using Cameras, processing their inputs frame by frame with some algorithm to achieve the deisired result.","Within these docs we will teach you how to implement things that we determine to be valuable within the scope of FIRST Robotics. More information on our roadmap"]},{"l":"What You Need","p":["To get started, we recommend a USB camera (anything around 720p or above will do), and a computer with Python 3, Pip, and an IDE of your choice to write it in. We recommend using VSCode if you are a beginner; it has a great plugin for python and a large community around it."]}],[{"l":"Cameras","p":["Cameras are the foundation of computer vision; without them, we wouldn't have images to process in the first place. Needless to say, at least a basic understanding is essential for working with vision code; thankfully, they're not as complicated as it seems."]},{"l":"Establishing the Type","p":["All digital cameras use a sensor and a lens to capture light from the environment. There are a couple of types of cameras, the main two that we'll focus on are fisheyes and pinholes. Generally, if you're thinking of a camera, it's a pinhole. Fisheyes are a lot more complicated, and involve a heap of extra work before they can be processed with computer vision. For most computer vision purposes, especially in FIRST robotics, pinhole cameras will be sufficient, and so that's what our articles will go in depth on."]},{"l":"Camera Properties","p":["Cameras have many different traits and properties that make them different. Some are extremely important when choosing a camera for your robot. Some could mean you cannot accomplish certain tasks. This means that effectively researching your cameras before purchasing can make (or break) your vision subsystems.","Here are the most essential properties to look for when finding a camera, in descending order:"]},{"i":"1-exposure-time","l":"**[1. Exposure Time]()**","p":["When your camera is capturing light to produce an image, the shutter needs to be open long enough to produce an image. This time ranges from camera to camera, but the higher it is, the more problems you'll have. This is because if your camera captures the light in the scene for a longer amount of time, there is more time for the scene to change, blurring the image. Generally, prioritize minimum exposure time when finding your camera."]},{"i":"2-resolution","l":"**[2. Resolution]()**","p":["The resolution of a camera is the amount of pixels in a single image produced by the camera. It is measured in pixels, typically represented as width x height. Having a low resolution is just like how nearsightedness works. When things are further away, they are harder to make out. This is because they are smaller from the camera's perspective, and because of this you have less pixels to work with. Essentially, the lower the resolution, the harder it is to make out small parts of an image."]},{"i":"3-field-of-view-fov","l":"**[3. Field Of View (FOV)]()**","p":["The FOV of a camera is the angular extent of which it can see at any given moment. It is determined by the camera's horizontal and vertical angle of view, often measured in degrees, which is determined by the cameras lens."]},{"i":"4-shutter-type-rolling-global","l":"**[4. Shutter Type (Rolling / Global)]()**","p":["The shutter type is the least important property of a camera if you prioritize the others first. Essentially, a camera will either capture all of the light at once( global shutter), or in a rolling motion ( rolling shutter), typically top to bottom. While a rolling shutter camera can cause some artifacts, it is not likely to be the sole cause."]},{"l":"Problems","p":["In a perfect world, any camera would suit any application. However, the cameras you will be purchasing for robotics will not be perfect. The first steps to dealing with this imperfection are determining the causes as well as the steps to improvement, which is exactly what this section will cover."]},{"i":"motion-blur","l":"**[Motion Blur]()**","p":["Motion blur is going to be the primary cause of your problems, whether it's that you cannot localize when you're turning, or that the slight shakiness of your robot moving causes blurriness. This is caused primarily by either too high of an exposure time or poor camera mounting."]},{"i":"rolling-shutter-artifacts","l":"**[Rolling Shutter Artifacts]()**","p":["Rolling shutter artifacts are a result of fast movement and a rolling shutter camera. These artifacts look like smearing, instead of bluriness like you would have with motion blur. You can mitigate rolling shutter artifacts by turning slower or reducing exposure time. You can also remove the though of them entirely by getting a global shutter camera.","Typically, a rolling shutter will not be a major source of issue, as majority of problems can be solved just by mitigating motion blur and getting a camera with a low exposure time."]},{"i":"mounting","l":"**[Mounting]()**","p":["Mounting a camera can seem like a trivial task, but it turns out to be quite a bit more complex than first assumed. Making sure your camera mount is stable, precise, and protects your camera is extremely important, because without, you will end up with innaccurate poses, motion blur, and worst of all, broken cameras."]}],[{"l":"Pinhole Cameras","p":["Pinhole cameras let light converge into a tiny hole (a pinhole!) and then diverge on the other side onto an image plane. In old cameras, this plane was the film. In digital cameras, it's a little more complicated, but the premise remains the same. This allows us to create this lovely diagram that forms the core of our vision systems:"]},{"l":"Interpreting the Triangles","p":["Ok, so what does that mean? This is a vertical side-view of a camera, where the intersection of the two dashed lines is the pinhole of the camera. The section of the horizontal dashed line on the camera's side (labelled as f) is the distance between the pinhole and the image plane.","The pinhole of the camera, where light converges, is known as a focal point, and the distance between it and the image plane is known as the focal length!","The image plane's height is marked as py, indicating p ixel y-axis length, and it represents a single column of pixels in the captured images.","This diagram doesn't have to be a vertical side-view, though; we can simply rename the py line to px and it is now a horizontal side-view, where the px line represents a single row of pixels in the captured image.","Don't get confused — we can't actually just say that py is the same as px; it's just a simple way to show that the camera's horizontal side-view shares proportionality with it's vertical side-view. When we put actual math into it, you'll have to consider the py and px sides separately."]},{"l":"Applications of the Triangles","p":["These triangles allow us to approximate the location of a target, provided that we can detect it with computer vision, and that we have some knowns about it. Let's draw in a few more things on the diagram:","That was a lot. Let's take it down part by part: the point labelled target is whatever we're targeting, possibly a piece of retroreflective tape on the wall? target (in image) indicates the position of the pixel that shows the target in the camera. You'll notice that it's upside down on the diagram; this is a side-effect of the pinhole structure.","The image gets flipped right-side up before it ever passes through to the code. However, drawing it like this makes it more intuitive to look at. Just remember that the py line is upside down!","Next, the segment marked h is the height of the target, relative to the camera's height (the distance vertically from the camera to the target). The segment marked d is the distance of the target from the camera laterally. The segment marked ph is the distance, in pixels, from the centre of the image to the pixel that the target appears on in the image.","So how do we use this diagram? Well, we can use the principle of similar triangles to solve for various unknowns: the triangle on the inside of the camera is proportional to that outside the camera. Here's what that looks like in an equation:","Since p_h and f are known ( p_h because it's in the image and f because it's constant and intrinsic to the camera), we can solve for either d or h as long as we know the other one. For a real-world application, if we know that the retroreflective tape is 8 feet off the ground, we can solve for how far away it is from the camera. For example's sake, let's say that it's pixel height is 200 px, and the camera's focal length is 678 px. We can solve for d by cross-multiplying:","It's as easy as that! No complicated trigonometry or linear algebra. Things get quite a bit harder when you don't know either distance or height, or if your target is more complex than a simple piece of retro tape or apriltag that can be represented by a single point."]}],[{"i":"#","p":["This section is a stub! Look for more in a future update."]},{"l":"Raspberry Pis","p":["The computer that most people use for vision in FRC is the Raspberry Pi, due to their ease of use, compact size, and relatively low price tag. Even though they may not have the greatest of processing power, they do well enough to be able to run computer vision for a robot! When setting up a pi, most will install the default Raspberry Pi operating system, a special flavour of Debian optimised for the Pi."]}],[{"l":"OpenCV","p":["OpenCV (Computer Vision) is a programming library that can help us achieve our goals when it comes to computer vision. As the name says, OpenCV is fully open source, and it handles much of the base level work with vision, including various aspects of image processing."]},{"l":"Installation","p":["In a virtual environment, you can run the following command to install OpenCV.","If you would like to know more about installation, visit the Python Packaging User Guide ."]},{"l":"How to Use","p":["OpenCV has documentation on all of its various functions and features, however, we obviously cannot cover all of them in these docs. Most of the important usages for OpenCV can be found on this page. We will try to cover all of the other important snippets when we need them, but if you want to go more in-depth, you can find their documentation in the OpenCV docs .","Anytime you would like to use OpenCV in a file, you'll need to import it with this line.","Keep in mind, this is not saying anytime we use an OpenCV function in the file, you need this before it. Importing OpenCV once, before any of your code will allow it to be used througout the whole file","There is no need to go looking too far into the docs, as we will show how to program any concepts we mention, but don't be afraid to look. The most important capabilities we will need OpenCV for are capturing video, changing camera properties, and inrange functions."]},{"l":"Capturing Video","p":["Capturing video from a camera is extremely easy with opencv, granted you're on your Raspberry Pi. OpenCV simply has two functions for these capabilities: cv2.VideoCapture() and VideoCapture.read(). With these two functions, you'll be able to get the image from your camera every frame. First, you must create a VideoCapture object using a line like this:","Now we have a VideoCapture object called cap that we can use to read from. In order to use the VideoCapture.read() function, we need to call it on a VideoCapture object, like we just created. Now, .read() will return whether or not the frame was captured, as well as the frame that was captured. We will need two variables to take these values, and simply written, you can implement this with:"]},{"l":"Showing Video","p":["If you have a monitor or screen and would like to view the image that your camera captured, you can use the cv2.imshow() function. For this function, you'll need to pass in a string for the name of the window (this does not matter unless you are showing multiple cameras), as well as the frame you would like to show. To add this to your program you would need a line like this inside the while loop:","However, we cannot just get away with this, because cv2.imshow() will not actually draw the image. We will need cv2.waitKey() to do that. This is because cv2.waitKey() will show an image until any keypress, or for a specified number of milliseconds, and then show the next frame registered with cv2.imshow(). This means that whenever you would like to show your image, you need cv2.waitKey(). This function will return the last keypress during that time period. There are two different ways to use the function:","Show for a number of milliseconds","In order to make the image display for a specified amount of time, you need to pass in a positive number to the function. For example, you can make each frame in a video feed show for 1ms by adding this line to the loop:","If you want to show the video capture until the user presses a key, you could modify it by adding an if statement. This loop will wait for the user to press 'q' and then breaks out of the loop.","This use of waitKey is very valuable for testing live video, though realistically you'd never want to show a frame for longer than 1ms.","Show indefinitely until any keypress","In order to make an image display until the user presses a key, you should pass in 0 to the function. This code, for example, will show the first image the camera captures (until the program ends), wait the user to give a keypress, and then end:","This usage of waitKey is much less valuable for our testing purposes, so you should stick with displaying an image for 1ms when testing your cameras or your code."]},{"l":"Changing Properties","p":["After you know how to capture video, you can fiddle around with your camera properties. Changing properties in OpenCV requires knowing a few things about you camera. You must know your camera's auto exposure setting ( auto_exposure), as well as the minimum exposure time ( exposure_time_absolute).","To find these, run v4l2-ctl -d /dev/videoX -L with X replaced with your camera. Typically this will be video0, but if you have multiple cameras you will have to determine which one is which. This command will give you an output of all of your changable camera properties, as well as their minimum, maximum, and default values. If a property has different modes specified by the number, it will list which number corresponds to which setting.","Once you know what value corresponds to Manual Mode for your auto exposure, as well as your camera's minimum exposure time, you may hop into programming.","In order to change a property with OpenCV, you must simply run the following line in your program, with the values substituted:","CAP_PROP_PROPERTY should be replaced with the property of your camera you would like to change. For exposure time, it is CAP_PROP_EXPOSURE. For auto exposure, it is CAP_PROP_AUTO_EXPOSURE. More property names and their definitions can be found in OpenCV's documentation . Also, val should be replaced with whatever value you would like to change it to.","If you would like to change the exposure time of your camera, you need to first change the auto exposure setting to manual. Otherwise, the value will not be changed."]}],[{"l":"Color Detection","p":["Object detection is the process of finding and localizing specific objects relative to the camera. In order to find an object, you have a lot of options. Here, we will describe how to detect objects using color, but you can also use neural network models or detect by shape. To detect an object based on its color, you need to consistently see the object in changing light conditions, as well as remove false positives. We can do this with OpenCV using some of their built-in functions."]},{"l":"Inrange and Bitwise Functions","p":["Some of the most basic yet essential functions in OpenCV are the inrange and bitwise functions. These work as follows:","cv2.inRange(src, lower_bound, upper_bound)- This function will take every single pixel of the specified source image ( src) and calculate whether or not it exists within a specified range ( lower_bound- upper_bound) with the bounds acting as allowed ranges for each color value. This will return a binary image with the 1s, or white pixels representing the pixels that fall within that range. This is most useful when used with an HSV color format. Here is an example use case of cv2.inRange that keeps all bright cyan pixels:","For each of these next functions, they all follow the same pattern of logic using boolean operators. Each one corresponds to their boolean operator and will take one or two images and return the boolean operator output of them combined. This should be done on binary images unless you are masking them.","cv2.bitwise_and(img_1, img_2, *mask)- Returns the boolean AND of the images.","cv2.bitwise_or(img_1, img_2, *mask)- Returns the boolean OR of the images.","cv2.bitwise_xor(img_1, img_2, *mask)- Returns the boolean XOR of the images.","cv2.bitwise_not(img_1, *mask)- Returns the boolean NOT of an image.","Each of these functions allows you to pass in a mask. This will allow you to pass in a binary image as a mask, to determine which pixels of the image will be kept, and which ones are removed regardless."]},{"l":"Dilation and Erosion","p":["After you have some sort of detection using inrange functions, you may start to notice two issues:","It is almost impossible to detect only a specific element using only inrange functions.","When you do have the object fully detected, there is a lot of noise","Fortunately, OpenCV has two functions that help us mitigate these problems. These functions are cv2.dilate() and cv2.erode(). These two methods will help get rid of small, unwanted detections, without scaling down the size of the desired detection. In order to get rid of noise, we use erode to remove detected pixels without a significant amount of other detected pixels around them. This will shrink the desired detection, so we use dilate to restore it to its original size. You may also do this in reverse order to get ride of \"holes\" in the detection.","Both cv2.erode() and cv2.dilate() will take in three arguments:","The base image","The kernel(this is a matrix filled with ones, to determine the buffer used for erosion and dilation)","The amount of iterations","When instantiating a kernel, it must be of odd size - (3, 5, 7)","Syntax: cv2.erode(image, kernel, iterations=iterations)","An example use case of erosion and dilation would be to first erode the image with a 3x3 kernel, for three iterations, and then reverse that with dilation:"]}],[{"l":"AprilTags"},{"i":"what-are-apriltags","l":"What are AprilTags?","p":["An Apriltag is a fiducial marker used for a variety of tasks, ranging from camera calibration to localization. AprilTags are all around the field, whether you are in FRC or FTC.","While the term fiducial might sound scary, it just means that the tag has a known, fixed position, serving as a point of comparison. This means whenever an AprilTag is seen by a camera, the camera can figure out its position relative to the tag."]},{"i":"how-do-we-use-apriltags","l":"How do we use Apriltags?","p":["As said before, there are a variety of useful implementations of AprilTags, and because of their properties, simply having a camera on your robot to detect the tags around the field is enough to improve your controls, localization and more."]},{"l":"Noteworthy Topics","p":["Before we go into AprilTag detection, there are a few things to keep in mind. First, be sure you have a decent camera setup.","You must also have a good camera calibration, as otherwise, your detections may be inaccurate, and therefore, worthless. It is a good idea to undistort your image before you attempt to detect any AprilTags.","Camera calibration articles coming in a future update!"]},{"l":"Detection","p":["at_detector.detect(img, estimate_tag_pose, camera_params, tag_size)","camera_params- Your camera parameters in the form [fx, fy, cx, cy]","decode_sharpening=0.25 This option helps to detect smaller tags.","Detecting AprilTags is quite complex, though it can be broken down into many steps. We will not go into the specifics of how tag detection works here, but you can view WPILib's explanation if you would like to know more.","estimate_tag_pose- Must be set to true.","families='tag36h11' This is the tag family of the FRC AprilTags, so you must use this setting.","For our implemetation, we will be using a library called dt_apriltags to give us the rotation and translation of the tag relative to our camera, as well as the tag ID. You can install it using pip:","If we were to create our Detector in code, it would look like this:","If we were to write this detector in code, it would look like this:","img- This must be your image in grayscale","In order to recieve detections from our camera, we need to call .detect() on our Detector. This is another function that has a few parameters:","In order to recieve detections, you must pass in these variables accordingly:","Most of these settings are default, so we do not need to set them all when we create our Detector.","nthreads=1 You can raise this up to 4 if you're using a Raspberry Pi 5, but you should do your own testing to see what works best.","quad_decimate=1.0 Changing this value to 1.0 will increase the detection rate of tags by increasing the resolution when searching for quads.","quad_sigma=0.0 Setting this value above 0 may help with noise, but if you do not have much noise, keep it at 0.","searchpath=['apriltags'] This is a required setting.","tag_size- The tag side length in meters. In FRC, the tags are 0.1651 x 0.1651m (6.5 inches).","To run detections, you must first create a Detector object. There are a couple of different options for this step, and we recommend finding your own settings for this, but we generally recommend the following values:"]}],[{"l":"Trigonometry","p":["Having a basic understanding of trigonometry is essential to understanding how to take data from an image and projecting it back into the real world. Trigonometry is the mathematics behind triangles, and mostly it gives us a few operations we can use on angles or side lengths to solve for the exact dimensions of a triangle."]},{"l":"SOH CAH TOA","p":["The three basic trig functions are \\sin, \\cos, and \\tan. Given the triangle:","We can define these functions as:","There are also inverse functions for these, so that you can determine the angle from the side lengths. These can be written in one of two ways: putting\\text{arc} in front, or by putting a ^{-1} right after the function, such as:","So as not to get \\sin^{-1} x confused with (\\sin x)^{-1}, there are another 3 functions that represent the reciprocals of the first three:","all of which can also be inverted by the same methods as above."]}],[{"l":"Linear Algebra","p":["Linear Algebra is an important tool that we use to help find the position of the robot on the field."]},{"l":"Vectors","p":["Vectors are used to represent a direction and magnitude. Usually they are graphically denoted by an arrow: the length represents the magnitude of the arrow and the starting and ending points represent the direction. On paper they are represented with square brackets [] with numbers vertically stacked. The top represents the x direction, below that is the y, and in 3 dimensions their will be z below that.","The above 2d vector represents an arrow with the x component being 10 units and the y component being two units, so you can imagine this as an arrow going from the point (0, 0) to the point (10, 2). In the context of robots, these vectors usually represent the position of something. For example, vectors are often used to represent where an april tag is from the camera."]},{"l":"Matrices","p":["In essence, matrices are a way to hold multiple pieces of information in a single entity.","Above is a two by two matrix. Graphically, however, representing them is a bit more complex. We generally think of matrices as transformations of the coordinate grid. Lets say we start with a vector","on the grid, but then we want to transform this vector using a matrix.","Applying this transformation, we get the x component, a, multilying the first column, and the y component, b, multiplying the second component:","and","combining these into a single vector we get our result vector:","This multi-step proccess is more commonly described as matrix multiplication. However, matrix multiplication isn't as simple as scalar multiplication, so you can't just multiply the corresponding components. We won't get into the details of matrix multiplication, but for more info on linear algebra and to get a much better visual representation of what has been explained in this article, We suggest watching this video series on youtube."]}],[{"l":"Driver Camera","p":["The simplest useful kind of vision program is one which just pushes an image to the driver station. We do this by using a simple mjpeg stream using mjpeg-streamer . To install, simply run:","You'll also need OpenCV. Then, the minimally functional program is:","That's a lot, so let's go chunk by chunk:","This creates a VideoCapture object around camera 0, but you should swap out 0 for whatever the camera id is. Finding this is notoriously difficult, but if all else fails, try -1, which should use the last available camera.","This creates a mjpeg stream with name stream_name and a specific size. You should change the size parameter to accurately represent the width and height of the images your camera takes in form (width, height).","This creates an mjpeg server hosted locally ( 127.0.0.1 is an ip address that refers to the current computer) on port 1181, accessible from a browser by xx.xx.xx.xx:1181, replacing the x s with the pi's IP address on the network. The stream is then added to the server, and then server is started.","127.0.0.1 will not work as the ip address on a computer that isn't the pi, since it also refers to the current computer on that computer, meaning it is expecting the stream to be hosted on that computer, not the pi!","Forever, this reads a frame from the camera and then pushes it through the stream.","If you run this program and then navigate to xx.xx.xx.xx:1181, as above, you should be able to see the stream! To stop it, you'll have to press ^C(control-C), potentially twice, to actually stop the program. This will cause it to get a little mad, so to fix that, we can do this instead:","The notable differences are wrapping the code in the function main, and then the if statement at the bottom: __name__ will be set to __main__ if the program is being run from the shell, but not if the file has been import ed into another python file; it's just generally a good practice. The try/ except KeyboardInterrupt will run main(), which loops until it receives an exception, including a KeyboardInterrupt event (you pressing ^C). By except ing and pass ing, we can \"gracefully\" catch the KeyboardInterrupt and end the program from there."]}],[{"i":"v20-δ--reefscape---pre-competition-update","l":"v2.0 δ | Reefscape - Pre-Competition Update","p":["This version of the site is δ (v2.0.0), released during the FIRST DIVE season and the Reefscape game (2025)."]},{"l":"Who We Are","p":["We are the vision subteam from FRC team 3656, the Dexter Dreadbots . Over time, new members will replace our old writers, and we plan on keeping this wiki up to date for as long as we still exist. We are always open to pull requests on our Github in case you would like to add an article. More information on contributing"]},{"l":"Changelog","p":["Bolded names are project leaders for that year."]},{"i":"v21-ε--reefscape---post-season-update--apr-20-2025","l":"v2.1 ε | Reefscape - Post-Season Update | Apr 20, 2025","p":["Examples and GIFs","Raspberry Pi Guide","PnP Localization","ChArUco Calibration","Installation Guides","Network Tables","Luke Baur","Fynn Nielsen","Join this list by contributing an article!"]},{"i":"v20-δ--reefscape---pre-competition-update--feb-1-2025","l":"v2.0 δ | Reefscape - Pre-Competition Update | Feb 1, 2025","p":["Page Authors","Contributing","Color Detection","AprilTags","OpenCV","Luke Baur","Fynn Nielsen"]},{"i":"v10-ɣ--crescendo-update--may-1-2024","l":"v1.0 ɣ | Crescendo Update | May 1, 2024","p":["Driver Camera Implementation","Basic Trig","Linear Algebra","Cameras","Calvin C Ophoff","Luke Baur","Fynn Nielsen","Arman Buyukbozkirlii"]},{"l":"All Contributors","p":["Luke Baur","Fynn Nielsen","Calvin C Ophoff","Arman Buyukbozkirlii","Cole Scheller(2022)","Josh Fernandez(2022)","Rose Dray"]},{"l":"How to Contribute","p":["If you would like to contribute to The Green Alliance and add an article, please open an issue on the Github , if one does not already exist for such an article. This way, you can communicate and collaborate with others while the article is in progress. If you would like to make an edit to an article, you can simply create a pull request with the change. Please remember that the Dreadbots have the final say for what goes on the site, but any and all relevant topics and contributions are welcome."]},{"l":"How to Make Edits","p":["The Green Alliance is powered by Retype , so please read their documentation to understand how it works. You can clone the repository and make local changes using retype, and after making your edits, open a pull request on the Github . If you are an author of a page, please make sure the page is configured as such (If you would like to be credited)."]},{"l":"Standardized Rules and Guidelines","p":["Before writing an article, please make sure you are adhering to the following rules and guidelines."]},{"l":"1. Keep it Vision","p":["The Green Alliance is about vision for FIRST Robotics , so all articles on The Green Alliance must be at least tangentially related to computer vision in a FIRST setting."]},{"l":"2. Follow the Styling","p":["Please try to stick to the format and styling seen on the site (Ex. All external links have this icon: , as well as open in a new tab). This will be edited when a page is added to the site, but it makes our job much easier."]},{"l":"3. This is a Community Resource","p":["Please remember that everything that goes on this site is available to anyone. We will not make any pages exclusive in any way, or block any team from contributing. There is no \"wrong solution\" when it comes to vision, so anything goes."]},{"l":"4. Only Homebrew Vision","p":["While there are systems (such as Limelight or PhotonVision) that do vision in FIRST, they have their own documentation and support and as such do not belong here. The Green Alliance is for people to learn and implement their own computer vision systems, and for teams to share and learn from eachothers vision solutions."]}]]