[[{"l":"The Green Alliance"},{"i":"what-is-computer-vision","l":"What is Computer Vision?","p":["Computer vision is the technology that allows computers to analyze, interpret, and understand visuals like humans do. It can involve various tasks ranging from spotting a saturated object to determining where something is in its environment. Computers complete these tasks by using Cameras, processing their inputs frame by frame with some algorithm to achieve the deisired result.","Within these docs we will teach you how to implement things that we determine to be valuable within the scope of FIRST Robotics. More information on our roadmap"]},{"l":"What You Need","p":["To get started, we recommend a USB camera (anything around 720p or above will do), and a computer with python 3, pip, and an IDE of your choice to write it in. We recommend using VSCode if you are a beginner; it has a great plugin for python and a large community around it."]}],[{"i":"#","p":["This section is a stub! Look out for more in a future update."]},{"l":"AprilTags"},{"i":"what-are-apriltags","l":"What are AprilTags?","p":["An Apriltag is a fiducial marker used for a variety of tasks, ranging from camera calibration to localization. AprilTags are all around the field, whether you are in FRC or FTC.","While the term fiducial might sound scary, it just means that the tag has a known fixed position, serving as a point of comparison. This means whenever an AprilTag is seen by a camera, the camera can figure out its position relative to the tag."]},{"i":"how-do-we-use-apriltags","l":"How do we use Apriltags?","p":["As said before, there are a variety of useful implementations of AprilTags, and because of their properties, simply having a camera on your robot to detect the tags around the field is enough to improve your controls, localization and more."]},{"l":"Noteworthy Topics","p":["Before we go into AprilTag detection, there are a few things to keep in mind. First, your camera. Unless your drivers move at the pace of a snail, you will need to minimize the motion blur of your camera. There are a few things that"]},{"l":"Detection"}],[{"l":"Math","p":["Without math, computer vision wouldn't exist. Though it may not be used to directly process the images, it forms the heart of the processing pipeline. It comes in many shapes and sizes, depending on the use case and efficiency of the operations; for instance, trigonometry is useful for mid-level processes, but basic geometry is sufficient for a lot of use cases (described briefly in Introduction to Pinholes). Eventually, we'll have articles on everything from simple geometry to linear algebra, but for now we'll start with beginning trigonometry."]}],[{"l":"Trigonometry","p":["Having a basic understanding of trigonometry is essential to understanding how to take data from an image and projecting it back into the real world. Trigonometry is the mathematics behind triangles, and mostly it gives us a few operations we can use on angles or side lengths to solve for the exact dimensions of a triangle."]},{"l":"SOH CAH TOA","p":["The three basic trig functions are \\sin, \\cos, and \\tan. Given the triangle:","We can define these functions as:","There are also inverse functions for these, so that you can determine the angle from the side lengths. These can be written in one of two ways: putting\\text{arc} in front, or by putting a ^{-1} right after the function, such as:","So as not to get \\sin^{-1} x confused with (\\sin x)^{-1}, there are another 3 functions that represent the reciprocals of the first three:","all of which can also be inverted by the same methods as above."]}],[{"l":"Linear Algebra","p":["Linear Algebra is an important tool that we use to help find the position of the robot on the field."]},{"l":"Vectors","p":["Vectors are used to represent a direction and magnitude. Usually they are graphically denoted by an arrow: the length represents the magnitude of the arrow and the starting and ending points represent the direction. On paper they are represent with square brackets [] with numbers vertically stacked. The top represents the x direction, below that is the y, and in 3 dimensions their will be z below that.","The above 2d vector represents an arrow with the x component being 10 units and the y component being two units, so you can imagine this as an arrow going from the point (0, 0) to the point (10, 2). In the context of robots, these vectors usually represent the position of something. For example, vectors are often used to represent where an april tag is from the camera."]},{"l":"Matrices","p":["In essence, matrices are a way to hold multiple pieces of information in a single entity.","Above is a two by two matrix. Graphically, however, representing them is a bit more complex. We generally think of matrices as transformations of the coordinate grid. Lets say we start with a vector","on the grid, but then we want to transform this vector using a matrix.","Applying this transformation, we get the x component, a, multilying the first column, and the y component, b, multiplying the second component:","and","combining these into a single vector we get our result vector:","This multi-step proccess is more commonly described as matrix multiplication. However, matrix multiplication isn't as simple as scalar multiplication, so you can't just multiply the corresponding components. I won't get into the details of matrx multiplication, but for more info on linear algebra and to get a much better visual representation of what has been explained in this article, I suggest watching this video series on youtube."]}],[{"l":"Cameras","p":["Cameras are the foundation of computer vision; without them, we wouldn't have images to process in the first place. Needless to say, at least a basic understanding is essential for working with vision code; thankfully, they're not as complicated as it seems."]},{"l":"Establishing the Type","p":["There are a couple of types of cameras, the main two that we'll focus on are fisheyes and pinholes. Generally, if you're thinking of a camera, it's a pinhole. Fisheyes are a lot more complicated, and involve a heap of extra work before they can be processed with computer vision. For most computer vision purposes, especially in FIRST robotics, pinhole cameras will be sufficient, and so that's what our articles will go in depth on."]},{"i":"how-do-cameras-work","l":"How do Cameras Work?","p":["Cameras use a sensor on the other side of their lens to convert recieved light into an image."]},{"l":"Camera Properties","p":["Cameras have many different traits and properties that make them different. Some are extremely important when choosing a camera for your robot. Some could mean you cannot accomplish certain tasks. This means that effectively researching your cameras before purchasing can make (or break) your vision subsystems.","Here are the most essential properties to look for when finding a camera, in descending order:"]},{"l":"1. Exposure Time","p":["When your camera is capturing light to produce an image, the shutter needs to be open long enough to produce an image. This time ranges from camera to camera, but the higher it is, the more problems you'll have"]},{"l":"2. Resolution","p":["The resolution of a camera is the amount of pixels in a single image produced by the camera. It is measured in pixels, typically represented as width x height. Having a low resolution is just like how nearsightedness works. When things are further away, they are harder to make out. This is because they are smaller from the camera's perspective, and because of this you have less pixels to work with. Essentially, the lower the resolution, the harder it is to make out small parts of an image."]},{"i":"3-field-of-view-fov","l":"3. Field Of View (**FOV**)","p":["The FOV of a camera is the angular extent of which it can see at any given moment. It is determined by the camera's horizontal and vertical angle of view, often measured in degrees, which is determined by the cameras lens."]},{"i":"4-shutter-type-rolling-global","l":"4. Shutter Type (Rolling / Global)","p":["The shutter type is the least important property of a camera if you prioritize the others first. Essentially, a camera will either capture all of the light at once( global shutter), or in a rolling motion ( rolling shutter), typically top to bottom. While a rolling shutter camera can cause some artifacts, it is not likely to be the sole cause."]},{"l":"Problems","p":["In a perfect world, any camera would suit any application. However, the cameras you will be purchasing for robotics will not be perfect. The first steps to dealing with this imperfection are determining the"]},{"l":"Motion Blur"}],[{"l":"Introduction to Pinholes","p":["Pinhole cameras let light converge into a tiny hole (a pinhole!) and then diverge on the other side onto an image plane. In old cameras, this plane was the film. In digital cameras, it's a little more complicated, but the premise remains the same. This allows us to create this lovely diagram that forms the core of our vision systems:"]},{"l":"Interpreting the Triangles","p":["Ok, so what the heck does that mean? This is a vertical side-view of a camera, where the intersection of the two dashed lines is the pinhole of the camera. The section of the horizontal dashed line on the camera's side (labelled as f) is the distance between the pinhole and the image plane.","The pinhole of the camera, where light converges, is known as a focal point, and the distance between it and the image plane is known as the focal length!","The image plane's height is marked as py, indicating p ixel y-axis length, and it represents a single column of pixels in the captured images.","This diagram doesn't have to be a vertical side-view, though; we can simply rename the py line to px and it is now a horizontal side-view, where the px line represents a single row of pixels in the captured image.","Don't get confused — we can't actually just say that py is the same as px; it's just a simple way to show that the camera's horizontal side-view shares proportionality with it's vertical side-view. When we put actual math into it, you'll have to consider the py and px sides separately."]},{"l":"Applications of the Triangles","p":["These triangles allow us to approximate the location of a target, provided that we can detect it with computer vision, and that we have some knowns about it. Let's draw in a few more things on the diagram:","That was a lot. Let's take it down part by part: the point labelled target is whatever we're targeting, possibly a piece of retroreflective tape on the wall? target (in image) indicates the position of the pixel that shows the target in the camera. You'll notice that it's upside down on the diagram; this is a side-effect of the pinhole structure.","The image gets flipped right-side up before it ever passes through to the code. However, drawing it like this makes it more intuitive to look at. Just remember that the py line is upside down!","Next, the segment marked h is the height of the target, relative to the camera's height (the distance vertically from the camera to the target). The segment marked d is the distance of the target from the camera laterally. The segment marked ph is the distance, in pixels, from the centre of the image to the pixel that the target appears on in the image.","So how do we use this diagram? Well, we can use the principle of similar triangles to solve for various unknowns: the triangle on the inside of the camera is proportional to that outside the camera. Here's what that looks like in an equation:","Since p_h and f are known ( p_h because it's in the image and f because it's constant and intrinsic to the camera), we can solve for either d or h as long as we know the other one. For a real-world application, if we know that the retroreflective tape is 8 feet off the ground, we can solve for how far away it is from the camera. For example's sake, let's say that it's pixel height is 200 px, and the camera's focal length is 678 px. We can solve for d by cross-multiplying:","It's as easy as that! No complicated trigonometry or linear algebra. Things get quite a bit harder when you don't know either distance or height, or if your target is more complex than a simple piece of retro tape or apriltag that can be represented by a single point."]}],[{"i":"#","p":["This section is a stub! Look for more in a future update."]},{"l":"Raspberry Pis","p":["The computer that most people use for computer vision in FRC is the raspberry pi, due to their ease of use, compact size, and relatively low price tag. Even though they may not have the greatest of processing power, they do well enough to be able to run computer vision for a robot! When setting up a pi, most will install the default Raspberry PI operating system, a special flavour of Debian optimised for the pi."]}],[{"i":"#","p":["This section is a stub! Look out for more in a future update."]},{"i":"what-is-opencv","l":"What is OpenCV?","p":["OpenCV (Computer Vision) is a programming library that can help us achieve our goals when it comes to computer vision. As the name says, OpenCV is fully open source, and it handles much of the base level work with vision, including various aspects of image processing."]},{"l":"Installation","p":["In a virtual environment, you can run the following command to install OpenCV.","If you would like to know more about installation, visit the Python Packaging User Guide ."]},{"l":"How to Use","p":["OpenCV has great documentation on all of its various functions and features, however, we obviously can't cover all of them in these docs. We will try to cover all of the important snippets when we need them, but if you want to go more in-depth, you can find their documentation in the OpenCV docs .","There is no need to go looking too far into the docs, as we will show how to program any concepts we mention, but don't be afraid to look."]}],[{"l":"Driver Camera","p":["The simplest useful kind of vision program is one which just pushes an image to the driver station. We do this by using a simple mjpeg stream using mjpeg-streamer . To install, simply run","You'll also need OpenCV. Then, the minimally functional program is:","That's a lot, so let's go chunk by chunk:","This creates a VideoCapture object around camera 0, but you should swap out 0 for whatever the camera id is. Finding this is notoriously difficult, but if all else fails, try -1, which should use the last available camera.","This creates a mjpeg stream with name stream_name and a specific size. You should change the size parameter to accurately represent the width and height of the images your camera takes in form (width, height).","This creates an mjpeg server hosted locally ( 127.0.0.1 is an ip address that refers to the current computer) on port 1181, accessible from a browser by xx.xx.xx.xx:1181, replacing the x s with the pi's IP address on the network. The stream is then added to the server, and then server is started.","127.0.0.1 will not work as the ip address on a computer that isn't the pi, since it also refers to the current computer on that computer, meaning it is expecting the stream to be hosted on that computer, not the pi!","Forever, this reads a frame from the camera and then pushes it through the stream.","If you run this program and then navigate to xx.xx.xx.xx:1181, as above, you should be able to see the stream! To stop it, you'll have to press ^C(control-C), potentially twice, to actually stop the program. This will cause it to get a little mad, so to fix that, we can do this instead:","The notable differences are wrapping the code in the function main, and then the if statement at the bottom: __name__ will be set to __main__ if the program is being run from the shell, but not if the file has been import ed into another python file; it's just generally a good practice. The try/ except KeyboardInterrupt will run main(), which loops until it receives an exception, including a KeyboardInterrupt event (you pressing ^C). By except ing and pass ing, we can \"gracefully\" catch the KeyboardInterrupt and end the program from there.","Congratulations, you just wrote your first vision program! Next, we'll look into detecting AprilTags, the new standard for marking FIRST fields."]}],[{"l":"Contributors"},{"l":"Our Mission","p":["Our mission is to provide beginner-friendly, easy-to-implement documentation and articles to help you implement hand written vision into your FIRST team's robot."]},{"l":"Who We Are","p":["We are the vision subteam from FRC team 3656, the Dexter Dreadbots . Over time, new members will replace our old writers, and we plan on keeping this wiki upto date for as long as we still exist. We are always open to pull requests on our Github in case you would like to add an article. More information on contributing can be found there.","Bolded names are project leaders for that year.","2022 (Pre-Launch): Cole Scheller, Calvin C Ophoff, Josh Fernandez, Rose Dray","2023 (Pre-Launch): Calvin C Ophoff, Rose Dray","2024 (Launch): Calvin C Ophoff, Luke Baur, Fynn Nielsen, Arman Buyukbozkirlii","2025 (Pre-ɣ): Luke Baur, Fynn Nielsen, Arman Buyukbozkirlii, Alex Scheller"]}],[{"i":"v031-pre-ɣ--crescendo-update","l":"v0.3.1 pre-ɣ | Crescendo Update","p":["This version of the site is Launch/pre-ɣ (pre-v1.0.0), released during the FIRST in Show season and the Crescendo game (2024)."]},{"l":"Changelog"},{"i":"v200-δ--post-crescendo-update--feb-1-2025","l":"v2.0.0 δ | Post-Crescendo Update | Feb 1, 2025","p":["Raspberry Pi Guide","Localization","Color Detection","Installation Guides","Network Tables","Camera Calibration"]},{"i":"v100-ɣ--crescendo-update--may-1-2024","l":"v1.0.0 ɣ | Crescendo Update | May 1, 2024","p":["Driver Camera Implementation","Basic Trig","Linear Algebra","AprilTags","OpenCV","Cameras"]}]]